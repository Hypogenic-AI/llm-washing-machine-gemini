\begin{abstract}
Large Language Models (LLMs) represent concepts as vectors in a high-dimensional space, but it remains unclear whether composite concepts (such as noun phrases) are represented as distinct, atomic directions or as linear compositions of their constituent parts. In this work, we investigate the representation of the concept ``washing machine'' in \gpt to determine if it occupies a unique orthogonal subspace or if it is constructed compositionally from the atomic concepts ``washing'' and ``machine.'' Using a synthetic dataset with strictly controlled contexts and linear probing of the residual stream, we find strong evidence for the compositional hypothesis. We show that the representation of the ``machine'' token in the context of ``washing machine'' remains extremely close to the generic ``machine'' vector (Cosine Similarity $> 0.98$). Furthermore, the deviation from the generic vector is significantly aligned with the ``washing'' concept vector (Cosine Similarity $0.35$). These results suggest that LLMs efficiently represent compound concepts by linearly adding modifier features to a stable head noun representation, rather than allocating dedicated capacity for every possible combination.
\end{abstract}
