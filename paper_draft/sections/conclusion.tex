\section{Conclusion}

In this work, we provided empirical evidence that \gpt represents the concept ``washing machine'' compositionally. By dissecting the residual stream, we showed that the model maintains a stable ``machine'' representation and modifies it by adding a vector aligned with the ``washing'' concept. This finding supports the Linear Representation Hypothesis for composite nouns and suggests that LLMs avoid the curse of dimensionality by leveraging the combinatorial structure of language in their latent space. Future work should investigate whether this linearity holds for less compositional or idiomatic phrases (e.g., ``red herring'') and across larger model scales.
