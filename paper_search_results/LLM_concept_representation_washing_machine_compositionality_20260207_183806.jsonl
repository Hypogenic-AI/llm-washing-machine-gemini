{"title": "Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs", "year": 2026, "authors": "Arya Labroo, Ivaxi Sheth, Vyas Raina, Amaani Ahmed, Mario Fritz", "url": "https://www.semanticscholar.org/paper/93ca5fe6af25507d3f13de5f86c9fcbfc8c50d5d", "relevance": 3, "abstract": "Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \\textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.", "citations": 0}
{"title": "From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks", "year": 2024, "authors": "Jacob Russin, Sam Whitman McGrath, Danielle J. Williams, Lotem Elber-Dorozko", "url": "https://www.semanticscholar.org/paper/9b0349ee9e68a3b958ec71a3d04092eee35ce97f", "relevance": 3, "abstract": "Compositionality has long been considered a key explanatory property underlying human intelligence: arbitrary concepts can be composed into novel complex combinations, permitting the acquisition of an open ended, potentially infinite expressive capacity from finite learning experiences. Influential arguments have held that neural networks fail to explain this aspect of behavior, leading many to dismiss them as viable models of human cognition. Over the last decade, however, modern deep neural networks (DNNs), which share the same fundamental design principles as their predecessors, have come to dominate artificial intelligence, exhibiting the most advanced cognitive behaviors ever demonstrated in machines. In particular, large language models (LLMs), DNNs trained to predict the next word on a large corpus of text, have proven capable of sophisticated behaviors such as writing syntactically complex sentences without grammatical errors, producing cogent chains of reasoning, and even writing original computer programs -- all behaviors thought to require compositional processing. In this chapter, we survey recent empirical work from machine learning for a broad audience in philosophy, cognitive science, and neuroscience, situating recent breakthroughs within the broader context of philosophical arguments about compositionality. In particular, our review emphasizes two approaches to endowing neural networks with compositional generalization capabilities: (1) architectural inductive biases, and (2) metalearning, or learning to learn. We also present findings suggesting that LLM pretraining can be understood as a kind of metalearning, and can thereby equip DNNs with compositional generalization abilities in a similar way. We conclude by discussing the implications that these findings may have for the study of compositionality in human cognition and by suggesting avenues for future research.", "citations": 7}
{"title": "Latent Concept Disentanglement in Transformer-based Language Models", "year": 2025, "authors": "Guan Zhe Hong, Bhavya Vasudeva, Vatsal Sharan, Cyrus Rashtchian, Prabhakar Raghavan, Rina Panigrahy", "url": "https://api.semanticscholar.org/CorpusId:279464131", "relevance": 3, "abstract": "When large language models (LLMs) use in-context learning (ICL) to solve a new task, they must infer latent concepts from demonstration examples. This raises the question of whether and how transformers represent latent structures as part of their computation. Our work experiments with several controlled tasks, studying this question using mechanistic interpretability. First, we show that in transitive reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. This builds upon prior work that analyzes single-step reasoning. Then, we consider tasks parameterized by a latent numerical concept. We discover low-dimensional subspaces in the model's representation space, where the geometry cleanly reflects the underlying parameterization. Overall, we show that small and large models can indeed disentangle and utilize latent concepts that they learn in-context from a handful of abbreviated demonstrations.", "citations": 2}
{"title": "Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning", "year": 2024, "authors": "Wenhan Chang, Tianqing Zhu, Heng Xu, Wenjian Liu, Wanlei Zhou", "url": "https://www.semanticscholar.org/paper/e8f60addab79851bc6488795cc4c4a6574b5d25a", "relevance": 3, "abstract": "In current AI era, users may request AI companies to delete their data from the training dataset due to the privacy concerns. As a model owner, retraining a model will consume significant computational resources. Therefore, machine unlearning is a new emerged technology to allow model owner to delete requested training data or a class with little affecting on the model performance. However, for large-scaling complex data, such as image or text data, unlearning a class from a model leads to a inferior performance due to the difficulty to identify the link between classes and model. An inaccurate class deleting may lead to over or under unlearning. In this paper, to accurately defining the unlearning class of complex data, we apply the definition of Concept, rather than an image feature or a token of text data, to represent the semantic information of unlearning class. This new representation can cut the link between the model and the class, leading to a complete erasing of the impact of a class. To analyze the impact of the concept of complex data, we adopt a Post-hoc Concept Bottleneck Model, and Integrated Gradients to precisely identify concepts across different classes. Next, we take advantage of data poisoning with random and targeted labels to propose unlearning methods. We test our methods on both image classification models and large language models (LLMs). The results consistently show that the proposed methods can accurately erase targeted information from models and can largely maintain the performance of the models.", "citations": 7}
{"title": "Improving Large Language Models with Concept-Aware Fine-Tuning", "year": 2025, "authors": "Michael Chen, Xikun Zhang, Jiaxing Huang, Dacheng Tao", "url": "https://api.semanticscholar.org/CorpusId:279251443", "relevance": 3, "abstract": "Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase\"ribonucleic acid\"as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments (\"rib\",\"on\", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm", "citations": 1}
{"title": "SkyReels-V2: Infinite-length Film Generative Model", "year": 2025, "authors": "Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zhenghao Chen, Chengchen Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhi-Xin Xu, Yuzhe Jin, Yu Liang, Yu-Ning Song, Peng Zhao, Bo Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, Yahui Zhou", "url": "https://www.semanticscholar.org/paper/e72efe88b16972f115f9dc85f0f2b165a9f4e6a9", "relevance": 1, "abstract": "Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs' inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation. To address these limitations, we propose SkyReels-V2, an Infinite-length Film Generative Model, that synergizes Multi-modal Large Language Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing Framework. Firstly, we design a comprehensive structural representation of video that combines the general descriptions by the Multi-modal LLM and the detailed shot language by sub-expert models. Aided with human annotation, we then train a unified Video Captioner, named SkyCaptioner-V1, to efficiently label the video data. Secondly, we establish progressive-resolution pretraining for the fundamental video generation, followed by a four-stage post-training enhancement: Initial concept-balanced Supervised Fine-Tuning (SFT) improves baseline quality; Motion-specific Reinforcement Learning (RL) training with human-annotated and synthetic distortion data addresses dynamic artifacts; Our diffusion forcing framework with non-decreasing noise schedules enables long-video synthesis in an efficient search space; Final high-quality SFT refines visual fidelity. All the code and models are available at https://github.com/SkyworkAI/SkyReels-V2.", "citations": 90}
{"title": "Compositional 3D-aware Video Generation with LLM Director", "year": 2024, "authors": "Hanxin Zhu, Tianyu He, Anni Tang, Junliang Guo, Zhibo Chen, Jiang Bian", "url": "https://www.semanticscholar.org/paper/c51f822d9fa458c298c6b3182e5c3a3d09ecd881", "relevance": 1, "abstract": "Significant progress has been made in text-to-video generation through the use of powerful generative models and large-scale internet data. However, substantial challenges remain in precisely controlling individual concepts within the generated video, such as the motion and appearance of specific characters and the movement of viewpoints. In this work, we propose a novel paradigm that generates each concept in 3D representation separately and then composes them with priors from Large Language Models (LLM) and 2D diffusion models. Specifically, given an input textual prompt, our scheme consists of three stages: 1) We leverage LLM as the director to first decompose the complex query into several sub-prompts that indicate individual concepts within the video~(\\textit{e.g.}, scene, objects, motions), then we let LLM to invoke pre-trained expert models to obtain corresponding 3D representations of concepts. 2) To compose these representations, we prompt multi-modal LLM to produce coarse guidance on the scales and coordinates of trajectories for the objects. 3) To make the generated frames adhere to natural image distribution, we further leverage 2D diffusion priors and use Score Distillation Sampling to refine the composition. Extensive experiments demonstrate that our method can generate high-fidelity videos from text with diverse motion and flexible control over each concept. Project page: \\url{https://aka.ms/c3v}.", "citations": 13}
{"title": "Exploring Multilingual Concepts of Human Values in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?", "year": 2024, "authors": "Shaoyang Xu, Weilong Dong, Zishan Guo, Xinwei Wu, Deyi Xiong", "url": "https://api.semanticscholar.org/CorpusId:268041734", "relevance": 1, "abstract": "Prior research has revealed that certain abstract concepts are linearly represented as directions in the representation space of LLMs, predominantly centered around English. In this paper, we extend this investigation to a multilingual context, with a specific focus on human values-related concepts (i.e., value concepts) due to their significance for AI safety. Through our comprehensive exploration covering 7 types of human values, 16 languages and 3 LLM series with distinct multilinguality (e.g., monolingual, bilingual and multilingual), we first empirically confirm the presence of value concepts within LLMs in a multilingual format. Further analysis on the cross-lingual characteristics of these concepts reveals 3 traits arising from language resource disparities: cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of value concepts. Moreover, we validate the feasibility of cross-lingual control over value alignment capabilities of LLMs, leveraging the dominant language as a source language. Ultimately, recognizing the significant impact of LLMs' multilinguality on our results, we consolidate our findings and provide prudent suggestions on the composition of multilingual data for LLMs pre-training.", "citations": 17}
{"title": "RACA: Representation-Aware Coverage Criteria for LLM Safety Testing", "year": 2026, "authors": "Zeming Wei, Zhixin Zhang, Chengcan Wu, Yihao Zhang, Xiaokun Luan, Meng Sun", "url": "https://www.semanticscholar.org/paper/d02eed7ef3820043251e26b5c5586e6ccc527c24", "relevance": 1, "abstract": "Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests. While coverage criteria have been effective for smaller neural networks, they are not directly applicable to LLMs due to scalability issues and differing objectives. To address these challenges, this paper introduces RACA, a novel set of coverage criteria specifically designed for LLM safety testing. RACA leverages representation engineering to focus on safety-critical concepts within LLMs, thereby reducing dimensionality and filtering out irrelevant information. The framework operates in three stages: first, it identifies safety-critical representations using a small, expert-curated calibration set of jailbreak prompts. Second, it calculates conceptual activation scores for a given test suite based on these representations. Finally, it computes coverage results using six sub-criteria that assess both individual and compositional safety concepts. We conduct comprehensive experiments to validate RACA's effectiveness, applicability, and generalization, where the results demonstrate that RACA successfully identifies high-quality jailbreak prompts and is superior to traditional neuron-level criteria. We also showcase its practical application in real-world scenarios, such as test set prioritization and attack prompt sampling. Furthermore, our findings confirm RACA's generalization to various scenarios and its robustness across various configurations. Overall, RACA provides a new framework for evaluating the safety of LLMs, contributing a valuable technique to the field of testing for AI.", "citations": 0}
{"title": "The Artificial Intelligence Ontology: LLM-Assisted Construction of AI Concept Hierarchies", "year": 2024, "authors": "marcin p. joachimiak, M. A. Miller, J. H. Caufield, Ryan Ly, Nomi L. Harris, Andrew Tritt, Christopher J. Mungall, Kristofer E. Bouchard", "url": "https://www.semanticscholar.org/paper/a75715361490ae49001e2b3377e40da3c4715ca5", "relevance": 1, "abstract": "The Artificial Intelligence Ontology (AIO) is a systematization of artificial intelligence (AI) concepts, methodologies, and their interrelations. Developed via manual curation, with the additional assistance of large language models (LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a comprehensive framework that encompasses both technical and ethical aspects of AI technologies. The primary audience for AIO includes AI researchers, developers, and educators seeking standardized terminology and concepts within the AI domain. We use the term \u201cbranches\u201d for classes, and their subclasses, in our ontology that are subclasses of owl:Thing. AIO contains eight branches: Bias, Layer, Machine Learning Task, Mathematical Function, Model, Network, Preprocessing, and Training Strategy, each designed to support the modular composition of AI methods and facilitate a deeper understanding of deep learning architectures and ethical considerations in AI. AIO uses the Ontology Development Kit (ODK) for its creation and maintenance, with its content being more easily updated through AI-driven curation support. This approach not only ensures the ontology's relevance amidst the fast-paced advancements in AI but also significantly enhances its utility for researchers, developers, and educators by simplifying the integration of new AI concepts and methodologies. The ontology's utility is demonstrated through the annotation of AI methods data in a catalog of AI research publications and the integration into the BioPortal ontology resource, highlighting its potential for cross-disciplinary research. The AIO ontology is open source and is available on GitHub (https://w3id.org/aio/) and BioPortal (https://bioportal.bioontology.org/ontologies/AIO).", "citations": 19}
{"title": "Beyond Syntax: How Do LLMs Understand Code?", "year": 2025, "authors": "Marc North, Amir Atapour-Abarghouei, Nelly Bencomo", "url": "https://www.semanticscholar.org/paper/3881aed342445910b32fb182dfa13d657300c565", "relevance": 1, "abstract": "Within software engineering research, Large Language Models (LLMs) are often treated as \u2018black boxes\u2019, with only their inputs and outputs being considered. In this paper, we take a machine interpretability approach to examine how LLMs internally represent and process code.We focus on variable declaration and function scope, training classifier probes on the residual streams of LLMs as they process code written in different programming languages to explore how LLMs internally represent these concepts across different programming languages. We also look for specific attention heads that support these representations and examine how they behave for inputs of different languages.Our results show that LLMs have an understanding \u2014 and internal representation \u2014 of language-independent coding semantics that goes beyond the syntax of any specific programming language, using the same internal components to process code, regardless of the programming language that the code is written in. Furthermore, we find evidence that these language-independent semantic components exist in the middle layers of LLMs and are supported by language-specific components in the earlier layers that parse the syntax of specific languages and feed into these later semantic components.Finally, we discuss the broader implications of our work, particularly in relation to concerns that AI, with its reliance on large datasets to learn new programming languages, might limit innovation in programming language design. By demonstrating that LLMs have a language-independent representation of code, we argue that LLMs may be able to flexibly learn the syntax of new programming languages while retaining their semantic understanding of universal coding concepts. In doing so, LLMs could promote creativity in future programming language design, providing tools that augment rather than constrain the future of software engineering.", "citations": 1}
{"title": "When Bad Data Leads to Good Models", "year": 2025, "authors": "Kenneth Li, Yida Chen, Fernanda Vi'egas, Martin Wattenberg", "url": "https://api.semanticscholar.org/CorpusId:278394812", "relevance": 1, "abstract": "In large language model (LLM) pretraining, data quality is believed to determine model quality. In this paper, we re-examine the notion of\"quality\"from the perspective of pre- and post-training co-design. Specifically, we explore the possibility that pre-training on more toxic data can lead to better control in post-training, ultimately decreasing a model's output toxicity. First, we use a toy experiment to study how data composition affects the geometry of features in the representation space. Next, through controlled experiments with Olmo-1B models trained on varying ratios of clean and toxic data, we find that the concept of toxicity enjoys a less entangled linear representation as the proportion of toxic data increases. Furthermore, we show that although toxic data increases the generational toxicity of the base model, it also makes the toxicity easier to remove. Evaluations on Toxigen and Real Toxicity Prompts demonstrate that models trained on toxic data achieve a better trade-off between reducing generational toxicity and preserving general capabilities when detoxifying techniques such as inference-time intervention (ITI) are applied. Our findings suggest that, with post-training taken into account, bad data may lead to good models.", "citations": 6}
{"title": "Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities", "year": 2026, "authors": "Dang Huu-Tien, The-Hai Nguyen, Dinh Mai Phuong, Nguyen Minh Phuong, Hoang Thanh-Tung, Le-Minh Nguyen, Naoya Inoue", "url": "https://www.semanticscholar.org/paper/d4fdb8564a67a9f92481dec8703b4b319e4cb971", "relevance": 1, "abstract": "We consider representation misdirection (RM), a class of LLM unlearning methods that achieves forgetting by manipulating the forget-representations, that is, latent representations of forget samples. Despite being important, the roles of target vectors used in RM, however, remain underexplored. Here, we approach and revisit RM through the lens of the linear representation hypothesis. Specifically, if one can somehow identify a one-dimensional representation corresponding to a high-level concept, the linear representation hypothesis enables linear operations on this concept vector within the forget-representation space. Under this view, we hypothesize that, beyond forgetting, machine unlearning elicits controllable side behaviors and stronger side capabilities corresponding to the high-level concept. Our hypothesis is empirically validated across a wide range of tasks, including behavioral control (e.g., controlling unlearned models'truth, sentiment, and refusal) and capability enhancement (e.g., improving unlearned models'in-context learning capability). Our findings reveal that this fairly attractive phenomenon could be either a hidden risk if misused or a mechanism that can be harnessed for developing models that require stronger capabilities and controllable behaviors.", "citations": 0}
{"title": "ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs", "year": 2025, "authors": "Zeming Wei, Chengcan Wu, Meng Sun", "url": "https://api.semanticscholar.org/CorpusId:279119635", "relevance": 1, "abstract": "Large Language Models (LLMs) have achieved significant success in various tasks, yet concerns about their safety and security have emerged. In particular, they pose risks in generating harmful content and vulnerability to jailbreaking attacks. To analyze and monitor machine learning models, model-based analysis has demonstrated notable potential in stateful deep neural networks, yet suffers from scalability issues when extending to LLMs due to their vast feature spaces. In this paper, we propose ReGA, a model-based analysis framework with representation-guided abstraction, to safeguard LLMs against harmful prompts and generations. By leveraging safety-critical representations, which are low-dimensional directions emerging in hidden states that indicate safety-related concepts, ReGA effectively addresses the scalability issue when constructing the abstract model for safety modeling. Our comprehensive evaluation shows that ReGA performs sufficiently well in distinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at the prompt level and 0.985 at the conversation level. Additionally, ReGA exhibits robustness to real-world attacks and generalization across different safety perspectives, outperforming existing safeguard paradigms in terms of interpretability and scalability. Overall, ReGA serves as an efficient and scalable solution to enhance LLM safety by integrating representation engineering with model-based abstraction, paving the way for new paradigms to utilize software insights for AI safety. Our code is available at https://github.com/weizeming/ReGA.", "citations": 3}
{"title": "Automating Cloud Infrastructure Provisioning with Semantically-Enriched Large Language Models", "year": 2025, "authors": "Weslley Paulo, B. Vasconcelos, Carlos Ferraz", "url": "https://www.semanticscholar.org/paper/37ef4eed79a88877226ae271cadaa7a9f2468cba", "relevance": 1, "abstract": "The complexity of provisioning multi-cloud infrastructure has created a significant automation bottleneck, and while Large Language Models (LLMs) offer a promising solution, they consistently fail to generate reliable and deployable Infrastructure as Code (IaC) due to inherent ambiguity. To address this critical reliability gap, we propose a novel methodology that significantly improves IaC generation by augmenting LLM prompts with structured semantic context. Our approach utilizes OWL ontologies to formally model key infrastructure concepts, grounding the LLM in a machine-readable representation of the domain. This semantic enrichment provides the specific, structured context needed to resolve ambiguity and enhance the accuracy of the generated Terraform code. We evaluate our approach on the IAC-EVAL benchmark, comparing our semantically-enriched method against standard prompting strategies. Experimental results demonstrate a definitive improvement: our approach achieves a mean functional accuracy of 64.3%, a 126.4% increase over the baseline average of 28.4%. Syntactic validity also improved dramatically, with Terraform plan validation rates increasing by an average of 29.6%. These findings showcase that formal semantic grounding is a critical and highly effective technique for building reliable, LLM-driven automation for complex cloud environments.", "citations": 0}
{"title": "LLM-Generated Class Descriptions for Semantically Meaningful Image Classification", "year": 2024, "authors": "Simone Bertolotto, Andr\u00e9 Panisson, Alan Perotti", "url": "https://www.semanticscholar.org/paper/11ed22746b8bc5c96538ad23caaced56764a066d", "relevance": 1, "abstract": ": Neural networks have become the primary approach for tackling computer vision tasks, but their lack of trans-parency and interpretability remains a challenge. Integrating neural networks with symbolic knowledge bases, which could provide valuable context for visual concepts, is not yet common in the machine learning community. In image classification, class labels are often treated as independent, orthogonal concepts, resulting in equal penalization of misclassifications regardless of the semantic similarity between the true and predicted labels. Previous studies have attempted to address this by using ontologies to establish relationships among classes, but such data structures are generally not available. In this paper, we use a large language model (LLM) to generate textual descriptions for each class label, aiming to capture the visual characteristics of the corresponding concepts. These descriptions are then encoded into embedding vectors, which are used as the ground truth for training the image classification model. By employing a cosine distance-based loss function, our approach considers the semantic similarity between class labels, encouraging the model to learn a more hierarchically structured internal feature representation. We evaluate our method on multiple datasets and compare its performance with existing techniques, focusing on classification accuracy, mistake severity, and the emergence of a hierarchical structure in the learned concept representations. The results suggest that semantic embedding representations extracted from LLMs have the potential to enhance the performance of image classification models and lead to more semantically meaningful misclassifications. A key advantage of our method, compared to those that leverage explicit hierarchical information, is its broad applicability to a wide range of datasets without requiring the presence of pre-defined hierarchical structures.", "citations": 1}
{"title": "Simulated Sense\u2010Making or Social Knowledge? Artificial Intelligence and the Boundaries of Representation", "year": 2025, "authors": "Lilian Negura", "url": "https://www.semanticscholar.org/paper/e88c8ff3f46725b32ef9e97d7a6d511ae52631da", "relevance": 1, "abstract": "This article examines whether AI\u2010generated texts\u2014such as stories produced by large language models (LLMs)\u2014can be considered social representations as defined by social representation theory. This paper argues that AI\u2010generated outputs simulate communicative behaviour without participating in social processes of meaning\u2010making. Although these texts contain familiar symbols, metaphors or narrative structures, they lack dialogical co\u2010construction, intentionality and embeddedness in cultural practices. This paper introduces the concept of quasi\u2010agents to capture the distinctive role that AI systems occupy in social interactions: entities perceived as social interlocutors, despite lacking genuine intentionality or social consciousness. This conceptual innovation extends social representation theory's analytical vocabulary, facilitating clearer distinctions between socially constructed meanings and algorithmically generated simulations. Misidentifying machine\u2010generated texts as genuine social knowledge risks eroding the dialogical foundations of public discourse, particularly in education, media and policy contexts. Ultimately, meaning\u2010making remains fundamentally a human and collective endeavour\u2014one that AI may mirror but not originate.", "citations": 1}
{"title": "Explainable LLM-based drone autonomy derived from partially observable geospatial data", "year": 2025, "authors": "Daniel Buffum, Jack Akers, J. Kerley, Derek T. Anderson, Andrew R. Buck, James M. Keller", "url": "https://www.semanticscholar.org/paper/2c333010d2303846586bd1cf68d852a7131d861b", "relevance": 1, "abstract": "Human teams are increasingly required to collaborate with machines (e.g., drones) in contexts that lack clearly defined formal rules for their interactions. These domains often demand that humans understand machine decisions and enable bidirectional communication to achieve optimal teamwork. In this work, we explore geospatial human-AI teaming where the AI constructs an uncertain partially observable spatio-temporal representation of its environment. We demonstrate how large language models (LLMs) can facilitate explainable autonomy, with our agent using an LLM for planning and navigation. The agent\u2019s decisions are based on summarized data layers from a probabilistic occupancy voxel map, which include factors such as elevation, exploration fringe, observation distance, and time since last observed. These layers, combined with any domain-specific directives, guide the agent\u2019s decision-making process. In addition to path planning, users can interact with the agent at any time to understand or alter its course of action. While this framework is demonstrated in a simulated environment to control variables and access known ground truth, it is designed to be transferable to real-world applications. We present several scenarios of increasing complexity, starting with a simple proof of concept using one-shot actions on a single feature layer, then adding multiple conflicting features with different plausible outcomes, and ending with a multi-step real-time simulation in a full 3D rendered environment.", "citations": 0}
{"title": "Beyond representation: rethinking intelligence in the age of LLMs", "year": 2025, "authors": "Johan Largo", "url": "https://www.semanticscholar.org/paper/60b390202fa7713c2efd3d95378776026fdffd90", "relevance": 1, "abstract": "", "citations": 1}
{"title": "Classification of Three-dimensional Electron Diffraction Data with a Large Language Model", "year": 2025, "authors": "Kazuyuki Yasuda, Masahito Kumagai, Masayuki Sato, Kazuhiko Komatsu, Hiroaki Kobayashi", "url": "https://www.semanticscholar.org/paper/a28b0d6558947cc392daeb8bb1c8fb601725c92e", "relevance": 1, "abstract": "Three-dimensional electron diffraction (3D ED) has become an essential technique for determining high-resolution molecular structures. It is particularly useful for extremely small crystals that cannot be analyzed using conventional X-ray diffraction. In typical 3D ED experiments, hundreds to thousands of molecular structures are reconstructed from a single sample. However, many of these structures are inaccurate due to factors such as low signal-to-noise ratios and the limited range of measurable angles in 3D ED. Traditionally, researchers have had to manually inspect each structure to identify valid ones, which is a time-consuming and labor-intensive process. We propose an LLM-centric automatic screening method to efficiently identify correct molecular structures from 3D ED outputs. The method begins by converting raw atomic coordinates into a topology-only language representation tailored to 3D ED data. Using this representation, the method proceeds through three stages: (1) rule-based filtering to eliminate clearly impossible candidates, (2) classification by a fine-tuned LLM trained on both correct and artificially-generated corrupted molecules, and (3) grouping to merge identical topologies. This combination allows diverse 3D ED datasets to be classified quickly and accurately. Experimental results obtained using seven real 3D ED datasets indicate that this method reduced the number of structures requiring manual inspection by an average of 95.8% while preserving every correct structure. This method substantially reduces the manual burden and enables efficient large-scale classification of 3D ED data.CCS Concepts\u2022 Computing methodologies \u2192 Machine learning approaches; Neural networks; \u2022 Applied computing \u2192 Computational chemistry.", "citations": 1}
{"title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems", "year": 2025, "authors": "Roberto Pugliese, G. Kourousias, Francesco Venier, Grazia Garlatti Costa", "url": "https://www.semanticscholar.org/paper/458cde32b03893c4fd7f2be74883bc0f156f5c51", "relevance": 1, "abstract": "The exponential growth of scientific literature presents significant challenges for researchers navigating the complex knowledge landscape. We propose\"Agentic Publications\", a novel LLM-driven framework complementing traditional publishing by transforming papers into interactive knowledge systems. Our architecture integrates structured data with unstructured content through retrieval-augmented generation and multi-agent verification. The framework offers interfaces for both humans and machines, combining narrative explanations with machine-readable outputs while addressing ethical considerations through automated validation and transparent governance. Key features include continuous knowledge updates, automatic integration of new findings, and customizable detail levels. Our proof-of-concept demonstrates multilingual interaction, API accessibility, and structured knowledge representation through vector databases, knowledge graphs, and verification agents. This approach enhances scientific communication across disciplines, improving efficiency and collaboration while preserving traditional publishing pathways, particularly valuable for interdisciplinary fields where knowledge integration remains challenging.", "citations": 0}
{"title": "Capturing Semantic Flow of ML-based Systems", "year": 2025, "authors": "Shin Yoo, Robert Feldt, Somin Kim, Naryeong Kim", "url": "https://www.semanticscholar.org/paper/6b22a89d2fc2f9257210d6fb400dea046073f14b", "relevance": 1, "abstract": "ML-based systems are software systems that incorporates machine learning components such as Deep Neural Networks (DNNs) or Large Language Models (LLMs). While such systems enable advanced features such as high performance computer vision, natural language processing, and code generation, their internal behaviour remain largely opaque to traditional dynamic analysis such as testing: existing analysis typically concern only what is observable from the outside, such as input similarity or class label changes. We propose semantic flow, a concept designed to capture the internal behaviour of ML-based system and to provide a platform for traditional dynamic analysis techniques to be adapted to. Semantic flow combines the idea of control flow with internal states taken from executions of ML-based systems, such as activation values of a specific layer in a DNN, or embeddings of LLM responses at a specific inference step of LLM agents. The resulting representation, summarised as semantic flow graphs, can capture internal decisions that are not explicitly represented in the traditional control flow of ML-based systems. We propose the idea of semantic flow, introduce two examples using a DNN and an LLM agent, and finally sketch its properties and how it can be used to adapt existing dynamic analysis techniques for use in ML-based software systems.", "citations": 0}
{"title": "SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?", "year": 2025, "authors": "Sanchit Kabra, Shobhnik Kriplani, Parshin Shojaee, Chandan K. Reddy", "url": "https://www.semanticscholar.org/paper/164b161d63f56a471ea22d10faf2e219edd4d03b", "relevance": 1, "abstract": "Equation discovery from data is a core challenge in machine learning for science, requiring the recovery of concise symbolic expressions that govern complex physical and geometric phenomena. Recent approaches with large language models (LLMs) show promise in symbolic regression, but their success often hinges on memorized formulas or overly simplified functional forms. Existing benchmarks exacerbate this limitation: they focus on scalar functions, ignore domain grounding, and rely on brittle string-matching based metrics that fail to capture scientific equivalence. We introduce SurfaceBench, first comprehensive benchmark for symbolic surface discovery. SurfaceBench comprises 183 tasks across 15 categories of symbolic complexity, spanning explicit, implicit, and parametric equation representation forms. Each task includes ground-truth equations, variable semantics, and synthetically sampled three dimensional data. Unlike prior SR datasets, our tasks reflect surface-level structure, resist LLM memorization through novel symbolic compositions, and are grounded in scientific domains such as fluid dynamics, robotics, electromagnetics, and geometry. To evaluate equation discovery quality, we pair symbolic checks with geometry-aware metrics such as Chamfer and Hausdorff distances, capturing both algebraic fidelity and spatial reconstruction accuracy. Our experiments reveal that state-of-the-art frameworks, while occasionally successful on specific families, struggle to generalize across representation types and surface complexities. SurfaceBench thus establishes a challenging and diagnostic testbed that bridges symbolic reasoning with geometric reconstruction, enabling principled benchmarking of progress in compositional generalization, data-driven scientific induction, and geometry-aware reasoning with LLMs. We release the code here: https://github.com/Sanchit-404/surfacebench", "citations": 0}
{"title": "Can Large Language Models (or Humans) Disentangle Text?", "year": 2024, "authors": "Nicolas Audinet de Pieuchon, Adel Daoud, C. Jerzak, Moa Johansson, Richard Johansson", "url": "https://www.semanticscholar.org/paper/a472078327b22bcd367ca30dbdcb32a10a508952", "relevance": 1, "abstract": "We investigate the potential of large language models (LLMs) to disentangle text variables\u2014to remove the textual traces of an undesired forbidden variable in a task sometimes known as text distillation and closely related to the fairness in AI and causal inference literature. We employ a range of various LLM approaches in an attempt to disentangle text by identifying and removing information about a target variable while preserving other relevant signals. We show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still detectable to machine learning classifiers post-LLM-disentanglement. Furthermore, we find that human annotators also struggle to disentangle sentiment while preserving other semantic content. This suggests there may be limited separability between concept variables in some text contexts, highlighting limitations of methods relying on text-level transformations and also raising questions about the robustness of disentanglement methods that achieve statistical independence in representation space.", "citations": 3}
{"title": "Towards Safe Concept Transfer of Multi-Modal Diffusion via Causal Representation Editing", "year": 2024, "authors": "Peiran Dong, Bingjie Wang, Song Guo, Junxiao Wang, Jie Zhang, Zicong Hong", "url": "https://www.semanticscholar.org/paper/ce909559b2d3e88139d0d2e0c7856eefbe3a1f6f", "relevance": 1, "abstract": "Recent advancements in vision-language-to-image (VL2I) diffusion generation have made significant progress. While generating images from broad vision-language inputs holds promise, it also raises concerns about potential misuse, such as copying artistic styles without permission, which could have legal and social consequences. Therefore, it\u2019s crucial to establish governance frameworks to ensure ethical and copyright integrity, especially with widely used diffusion models. To address these issues, researchers have explored various approaches, such as dataset filtering, adversarial perturbations, machine unlearning, and inference-time refusals. However, these methods often lack either scalability or effectiveness. In response, we propose a new framework called causal representation editing (CRE), which extends representation editing from large language models (LLMs) to diffusion-based models. CRE enhances the efficiency and flexibility of safe content generation by intervening at diffusion timesteps causally linked to unsafe concepts. This allows for precise removal of harmful content while preserving acceptable content quality, demonstrating superior effectiveness, precision and scalability compared to existing methods. CRE can handle complex scenarios, including incomplete or blurred representations of unsafe concepts, offering a promising solution to challenges in managing harmful content generation in diffusion-based models.", "citations": 2}
{"title": "The Concept of Analogy in the Qur'an: Hudan Representation in IT Perspective (Information TECHNOLOGY)", "year": 2024, "authors": "Khotimatul Maulidah", "url": "https://www.semanticscholar.org/paper/4c6aff51d9843160b2adb5dbe4cd268927203a7f", "relevance": 1, "abstract": "The Qur\u2019an encompasses a wide range of knowledge, including the Qur\u2019anic proverbs found in its text. These proverbs of the Qur\u2019an offer concise and eloquent expressions, making abstract ideas accessible and relatable. The proverbs of the Qur\u2019an become valuable lessons for those who want to learn them. The approach used is the interpretation and analogy approach. As a result, this study states that; First, the concept of hudan in the Qur\u2019an has various meanings, depending on the context of the verse that explains it. When divided into several forms, the concept of hudan in the Qur\u2019an is divided into forms, such as; classification of hudan, level of hudan, criteria for people who get hudan, and causes of obstruction for someone to get hudan. Second, the concept of hudan in the Qur\u2019an if analogized in the context of IT media (information technology) then it can be likened to the use of washing machines in the context of; functions and objectives, operationalization, and causes of not running optimally.", "citations": 1}
{"title": "The Issues of Creation of Machine-Understandable Smart Standards Based on Knowledge Graphs", "year": 2024, "authors": "E. Shalfeeva, V. Gribova", "url": "https://www.semanticscholar.org/paper/53da79299493079bd30c91d6b8aa951b142ac729", "relevance": 1, "abstract": "The development of digital transformation requires the widespread use of digital technologies in standardization documents. One of the goals is to create standards with machine-understandable content that will allow the use of digital documents at various stages of development and production without the need for a human operator. The purpose of this work is to describe an approach for creating and translating industry normative documents into a machine-understandable representation for their further use in software services and systems. There are three types of SMART standard content: machine-readable, machine-interpretable, and machine-understandable. Knowledge graphs are actively used to formalize data and knowledge when solving various problems. The new two-level approach is proposed for the creation and translation into a machine-understandable representation of regulatory documents as knowledge graphs. The approach defines two types of interpretation of a smart document (human readability and machine understandability) through two related formats: a graph, each semantic node of which represents text in a natural language, and a network of concepts and strict connections. Each node of a human-readable graph corresponds (in general) to a subtree of a machine-readable knowledge graph. As the basis for ensuring the transformation of one form of smart standard representation into another form, LLM models are used, supplemented by a specialized adapter obtained as a result of additional training using the Parameter-Efficient Fine-Tuning approach. Requirements have been established for a set of problem- and subject-oriented tools for generating knowledge graphs. The conceptual architecture of the system for supporting the solution of a set of problems based on knowledge graphs is shown, and the principles for implementing software components that work with smart knowledge for intelligent software services are established.", "citations": 0}
{"title": "The Representation of Meaningful Precision, and Accuracy", "year": 2024, "authors": "A. Mani", "url": "https://www.semanticscholar.org/paper/34618508377e2682d7ef28e03f224d5a3f676bed", "relevance": 1, "abstract": "The concepts of precision, and accuracy are domain and problem dependent. The simplified numeric hard and soft measures used in the fields of statistical learning, many types of machine learning, and binary or multiclass classification problems are known to be of limited use for understanding the meaningfulness of models or their relevance. Arguably, they are neither of patterns nor proofs. Further, there are no good measures or representations for analogous concepts in the cognition domain. In this research, the key issues are reflected upon, and a compositional knowledge representation approach in a minimalist general rough framework is proposed for the problem contexts. The latter is general enough to cover most application contexts, and may be applicable in the light of improved computational tools available.", "citations": 0}
{"title": "Editorial: Modularity in motor control: from muscle synergies to cognitive action representation", "year": 2015, "authors": "A. d\u2019Avella, M. Giese, Y. Ivanenko, T. Schack, T. Flash", "url": "https://www.semanticscholar.org/paper/eb26af850bd21497d1158162272b3fba5654c36f", "relevance": 1, "abstract": "Mastering a rich repertoire of motor behaviors, as humans and other animals do, is a surprising and still a poorly understood outcome of evolution, development, and learning. Many degrees-of-freedom, non-linear dynamics, and sensory delays provide formidable challenges for controlling even simple actions. Modularity as a functional element, both structural and computational, of a control architecture might be the key organizational principle that the central nervous system employs for achieving versatility and adaptability in motor control. Recent investigations of muscle synergies, motor primitives, compositionality, basic action concepts, and related work in machine learning have contributed, at different levels, to advance our understanding of the modular architecture underlying rich motor behaviors. \n \nHowever, the existence and nature of the modules comprising the control architecture is far from settled. For instance, regularity and low-dimensionality of the motor output are often taken as an indication of modularity but they could simply be a byproduct of optimization and task constraints. Moreover, what are the relationships between modules at different levels, such as muscle synergies, kinematic invariants, and basic action concepts? \n \nOne important reason for the new interest in understanding modularity in motor control from different perspectives is the impressive development in cognitive robotics. In comparison to animals and humans, the motor skills of today's best robots are limited and inflexible. However, robot technology is maturing to the point at which it can start approximating a reasonable spectrum of different perceptual, cognitive, and motor capabilities. These advances allow researchers to explore how these motor, sensory, and cognitive functions might be integrated into meaningful architectures and to test their functional limits. Such systems provide a new test bed to explore different concepts of modularity and to experimentally investigate possible interactions between motor and cognitive processes. \n \nThus, the goal of this Research Topic is to review, compare, and debate theoretical and experimental studies of the modular organization of the motor control system at different levels. By bringing together researchers seeking to understand the building blocks of coordinating many muscles, planning endpoint and joint trajectories, and representing motor and behavioral actions in memory we aim at promoting new interactions between often disconnected research areas and approaches and providing a broad perspective on the notion of modularity in motor control.", "citations": 68}
{"title": "Towards more human-like concept learning in machines : compositionality, causality, and learning-to-learn", "year": 2014, "authors": "B. Lake", "url": "https://www.semanticscholar.org/paper/e953da04138c64f5d7d0ba6d24e621fe15cb8ecc", "relevance": 1, "abstract": "", "citations": 44}
{"title": "A Knowledge Representation Tool for Autonomous Machine Learning Based on Concept Algebra", "year": 2009, "authors": "Yousheng Tian, Yingxu Wang, Kai Hu", "url": "https://www.semanticscholar.org/paper/deff2b92fa5b8076af5e9f9a6dca013813a5a793", "relevance": 1, "abstract": "", "citations": 18}
{"title": "New Functional Representation for the Decomposition of Machine Learning Problems", "year": 2000, "authors": "C. Files", "url": "https://www.semanticscholar.org/paper/b70fc376ff7dc96e73a70e7df59299547f40d74f", "relevance": 1, "abstract": "", "citations": 2}
{"title": "The 3rd Workshop on Continuous Vector Space Models and Their Compositionality (cvsc) Recursive Neural Networks Can Learn Logical Semantics Joint Semantic Relevance Learning with Text Data and Graph Knowledge Exploring the Effect of Semantic Similarity for Phrase-based Machine Translation Incremental", "year": null, "authors": "Edward Grefenstette, Google DeepMind, Karl Moritz, Hermann, Alexandre Allauzen, Marco Baroni, L. Bottou, Stephen Clark, Jason Weston, Guillaume Wisniewski, Raymond Mooney, Percy Liang, Stanford University, Samuel R. Bowman, Christopher Potts, Christopher D, Dongxu Zhang, Bin Yuan, Dong Wang, Rong, Alex Ter-Sarkisov, H. Schwenk, Fethi Bougares, Friday, Ix Friday, Christopher D. Manning, Jackie Chi, Kit Cheung, Rong Liu, Kunal Sachdeva, Dipti Sharma, Lo\u00efc Barrault, Kristina Toutanova, Danqi Chen, Kazuma Hashimoto, Yoshimasa Tsuruoka", "url": "https://www.semanticscholar.org/paper/e9884e185f29e2148095f134573b4b41dad220e7", "relevance": 1, "abstract": "", "citations": 1}
